# Налаштування AI чату

!!! info
    AI чат вимагає Gramps Web API версії 2.5.0 або вище.


Gramps Web API підтримує можливість ставити запитання про генеалогічну базу даних, використовуючи великі мовні моделі (LLM) через техніку, звану генерацією з підсиленням запитів (RAG).

RAG працює наступним чином. Спочатку використовується *модель векторного вбудовування*, щоб створити індекс усіх об'єктів у базі даних Gramps у формі числових векторів, які кодують значення об'єктів. Цей процес подібний до створення індексу повнотекстового пошуку, але є більш витратним з точки зору обчислень.

Далі, коли користувач ставить запитання через кінцеву точку чату, це запитання також перетворюється на вектор тією ж моделлю вбудовування і порівнюється з об'єктами в базі даних Gramps. Цей *семантичний пошук* поверне об'єкти в базі даних, які є найбільш семантично подібними до запитання.

На фінальному етапі запитання та отримані об'єкти надсилаються до LLM для формулювання відповіді на основі наданої інформації. Таким чином, чат-бот має доступ до детальної інформації про вміст генеалогічної бази даних, а не покладається лише на попередньо існуючі знання.

Щоб активувати кінцеву точку чату в Gramps Web API, необхідно виконати три кроки:

1. Встановлення необхідних залежностей,
2. Увімкнення семантичного пошуку,
3. Налаштування постачальника LLM.

Ці три кроки описані нижче по черзі. Нарешті, власник або адміністратор повинен [налаштувати, які користувачі можуть отримати доступ до функції чату](users.md#configuring-who-can-use-ai-chat) у налаштуваннях Керування користувачами.

## Встановлення необхідних залежностей

AI чат вимагає встановлення бібліотек Sentence Transformers та PyTorch.

Стандартні образи Docker для Gramps Web вже мають їх попередньо встановленими для архітектур `amd64` (наприклад, 64-розрядний настільний ПК) та `arm64` (наприклад, 64-розрядний Raspberry Pi). На жаль, AI чат не підтримується на архітектурі `armv7` (наприклад, 32-розрядний Raspberry Pi) через відсутність підтримки PyTorch.

При встановленні Gramps Web API через `pip` (це не потрібно при використанні образів Docker) необхідні залежності встановлюються за допомогою

```bash
pip install gramps_webapi[ai]
```

## Увімкнення семантичного пошуку

Якщо необхідні залежності встановлені, увімкнення семантичного пошуку може бути таким простим, як налаштування параметра конфігурації `VECTOR_EMBEDDING_MODEL` (наприклад, шляхом налаштування змінної середовища `GRAMPSWEB_VECTOR_EMBEDDING_MODEL`), див. [Конфігурація сервера](configuration.md). Це може бути будь-який рядок моделі, підтримуваної бібліотекою [Sentence Transformers](https://sbert.net/). Дивіться документацію цього проекту для отримання деталей та доступних моделей.

!!! warning
    Зверніть увагу, що стандартні образи Docker не включають версію PyTorch з підтримкою GPU. Якщо у вас є доступ до GPU (що значно прискорить семантичну індексацію), будь ласка, встановіть версію PyTorch з підтримкою GPU.

Існує кілька міркувань, які слід врахувати при виборі моделі.

- Коли ви змінюєте модель, вам потрібно вручну відтворити семантичний пошуковий індекс для вашого дерева (або всіх дерев у багатодеревній конфігурації), інакше ви зіткнетеся з помилками або безглуздими результатами.
- Моделі є компромісом між точністю/загальністю з одного боку та обчислювальним часом/місцем для зберігання з іншого. Якщо ви не запускаєте Gramps Web API на системі, яка має доступ до потужного GPU, більші моделі зазвичай занадто повільні на практиці.
- Якщо ваша вся база даних не англійською мовою і всі ваші користувачі не очікують ставити запитання в чаті лише англійською, вам знадобиться багатомовна модель вбудовування, які є більш рідкісними, ніж чисто англійські моделі.

Якщо модель не присутня в локальному кеші, вона буде завантажена, коли Gramps Web API буде запущено вперше з новою конфігурацією. Модель `sentence-transformers/distiluse-base-multilingual-cased-v2` вже доступна локально при використанні стандартних образів Docker. Ця модель є хорошою відправною точкою і підтримує багатомовний ввід.

Будь ласка, діліться знаннями про різні моделі з громадою!

!!! info
    Бібліотека sentence transformers споживає значну кількість пам'яті, що може призвести до завершення роботи робочих процесів. Як правило, з увімкненим семантичним пошуком кожен робочий процес Gunicorn споживає близько 200 МБ пам'яті, а кожен робочий процес celery - близько 500 МБ пам'яті навіть у бездіяльності, і до 1 ГБ під час обчислення вбудовувань. Дивіться [Обмеження використання CPU та пам'яті](cpu-limited.md) для налаштувань, які обмежують використання пам'яті. Крім того, рекомендується забезпечити достатньо великий розділ підкачки, щоб запобігти помилкам OOM через сплески використання пам'яті.

## Налаштування постачальника LLM

Зв'язок з LLM використовує API, сумісний з OpenAI, за допомогою бібліотеки `openai-python`. Це дозволяє використовувати локально розгорнуту LLM через Ollama (див. [Сумісність Ollama з OpenAI](https://ollama.com/blog/openai-compatibility)) або API, такі як OpenAI або Hugging Face TGI (Text Generation Inference). LLM налаштовується через параметри конфігурації `LLM_MODEL` та `LLM_BASE_URL`.

### Використання хостингу LLM через OpenAI API

При використанні OpenAI API `LLM_BASE_URL` можна залишити незадано, тоді як `LLM_MODEL` потрібно налаштувати на одну з моделей OpenAI, наприклад, `gpt-4o-mini`. Зверніть увагу, що через підхід RAG LLM використовується "лише" для вибору правильної інформації з результатів семантичного пошуку та формулювання відповіді, він не вимагає глибоких генеалогічних або історичних знань. Тому ви можете спробувати, чи достатньо маленької/дешевої моделі.

Вам також потрібно буде зареєструвати обліковий запис, отримати API-ключ і зберегти його в змінній середовища `OPENAI_API_KEY`.

!!! info
    `LLM_MODEL` є параметром конфігурації; якщо ви хочете налаштувати його через змінну середовища, використовуйте `GRAMPSWEB_LLM_MODEL` (див. [Конфігурація](configuration.md)). `OPENAI_API_KEY` не є параметром конфігурації, а є змінною середовища, яка безпосередньо використовується бібліотекою `openai-python`, тому її не слід префіксувати.

### Використання локальної LLM через Ollama

[Ollama](https://ollama.com/) є зручним способом запуску LLM локально. Будь ласка, ознайомтеся з документацією Ollama для отримання деталей. Зверніть увагу, що LLM вимагають значних обчислювальних ресурсів, і всі, крім найменших моделей, ймовірно, будуть занадто повільними без підтримки GPU. Ви можете спробувати, чи відповідає [`tinyllama`](https://ollama.com/library/tinyllama) вашим потребам. Якщо ні, спробуйте одну з більших моделей. Будь ласка, діліться будь-яким досвідом з громадою!

При розгортанні Gramps Web з Docker Compose ви можете додати сервіс Ollama

```yaml
services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

volumes:
    ollama_data:
```

і потім налаштувати параметр конфігурації `LLM_BASE_URL` на `http://ollama:11434/v1`. Налаштуйте `LLM_MODEL` на модель, підтримувану Ollama, і завантажте її у ваш контейнер за допомогою `ollama pull <model>`. Нарешті, налаштуйте `OPENAI_API_KEY` на `ollama`.

Щоб усунути проблеми з Ollama, ви можете увімкнути налагоджувальне ведення журналу, налаштувавши змінну середовища `OLLAMA_DEBUG=1` в середовищі сервісу Ollama.

!!! info
    Якщо ви використовуєте Ollama для AI чату Gramps Web, будь ласка, підтримайте громаду, доповнивши цю документацію будь-якими відсутніми деталями.

### Використання інших постачальників

Будь ласка, не соромтеся надсилати документацію для інших постачальників і ділитися своїм досвідом з громадою!
